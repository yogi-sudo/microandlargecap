#!/usr/bin/env python3
import os
import datetime as dt
from typing import List
import numpy as np
import pandas as pd
import yfinance as yf

# Folders (match your project layout)
CACHE_DIR = os.getenv("CACHE_DIR", "cache")
OUT_DIR   = os.getenv("OUT_DIR", "out")
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(OUT_DIR, exist_ok=True)

# Defaults (overridable via env OR function args)
DEFAULT_YEARS    = int(os.getenv("TRAIN_YEARS", 3))
DEFAULT_MIN_ROWS = int(os.getenv("MIN_ROWS", 150))
DEFAULT_MIN_PX   = float(os.getenv("MIN_PRICE", 0.2))
DEFAULT_MIN_VOL  = float(os.getenv("W_MIN_VOL", 10_000))  # avg shares/day
DEFAULT_UNI_MAX  = int(os.getenv("UNIVERSE_MAX", 0))      # 0 = no cap

TODAY = dt.date.today()

def _cache_path(ticker: str) -> str:
    return os.path.join(CACHE_DIR, f"{ticker}_ohlc.csv")

def fetch_prices(ticker: str, years: int = DEFAULT_YEARS) -> pd.DataFrame:
    """
    Returns OHLCV with columns: date, open, high, low, close, volume (all numeric), ascending by date.
    Uses cache if present, otherwise downloads via yfinance and writes cache.
    """
    fn = _cache_path(ticker)
    # Try cache first
    if os.path.exists(fn):
        try:
            df = pd.read_csv(fn, parse_dates=["date"])
            need = {"date", "open", "high", "low", "close", "volume"}
            if not df.empty and need.issubset(df.columns):
                # ensure numerics
                for c in ["open", "high", "low", "close", "volume"]:
                    df[c] = pd.to_numeric(df[c], errors="coerce")
                df = df.dropna(subset=["date", "close", "volume"])
                return df.sort_values("date").reset_index(drop=True)
        except Exception:
            pass

    # Download fresh
    start = TODAY - dt.timedelta(days=365 * years + 7)
    end   = TODAY + dt.timedelta(days=1)
    y = yf.download(ticker, start=start.isoformat(), end=end.isoformat(), progress=False)
    if y is None or y.empty:
        return pd.DataFrame()

    y = y.reset_index().rename(columns={
        "Date": "date",
        "Open": "open",
        "High": "high",
        "Low":  "low",
        "Close":"close",
        "Volume":"volume"
    })
    keep = ["date", "open", "high", "low", "close", "volume"]
    for c in keep:
        if c != "date":
            y[c] = pd.to_numeric(y[c], errors="coerce")
    y = y.dropna(subset=["date", "close", "volume"])[keep].copy()
    try:
        y.to_csv(fn, index=False)
    except Exception:
        # best effort cache write
        pass
    return y.sort_values("date").reset_index(drop=True)

def build_dataset(
    tickers: List[str],
    years: int = None,
    min_rows: int = None,
    min_price: float = None,
    min_vol: float = None,
    universe_max: int = None,
) -> pd.DataFrame:
    """
    Build a stacked OHLCV dataframe for a list of tickers, with basic filtering.
    Accepts keyword arguments (so main.py can call build_dataset(tickers=...)).

    Returns columns: ticker, date, open, high, low, close, volume
    """
    years       = DEFAULT_YEARS    if years is None else years
    min_rows    = DEFAULT_MIN_ROWS if min_rows is None else min_rows
    min_price   = DEFAULT_MIN_PX   if min_price is None else min_price
    min_vol     = DEFAULT_MIN_VOL  if min_vol is None else min_vol
    universe_max= DEFAULT_UNI_MAX  if universe_max is None else universe_max

    if universe_max and universe_max > 0:
        tickers = tickers[:universe_max]

    frames = []
    for i, t in enumerate(tickers, 1):
        px = fetch_prices(t, years=years)
        print(f"\rDownload: {i}/{len(tickers)}", end="", flush=True)
        if px.empty or len(px) < min_rows:
            continue
        # Filters
        last_close = float(px["close"].iloc[-1])
        if last_close < min_price:
            continue
        v20 = px["volume"].rolling(20).mean().iloc[-1]
        if pd.isna(v20) or v20 < min_vol:
            continue

        px = px.copy()
        px["ticker"] = t
        frames.append(px)

    print()
    if not frames:
        raise SystemExit("No usable histories after filters. Lower MIN_ROWS/W_MIN_VOL or widen universe.")
    data = pd.concat(frames, ignore_index=True)
    # Normalize dtypes
    data["date"] = pd.to_datetime(data["date"])
    for c in ["open","high","low","close","volume"]:
        data[c] = pd.to_numeric(data[c], errors="coerce")
    data = data.dropna(subset=["date","close","volume"])
    return data.sort_values(["ticker","date"]).reset_index(drop=True)

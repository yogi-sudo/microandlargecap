#!/usr/bin/env python3
import os, sys, datetime as dt, json, math
import pandas as pd, numpy as np
from tqdm import tqdm
import requests
from argparse import ArgumentParser

# -------------------- Config (env overridable) --------------------
EODHD_API_KEY = os.getenv("EODHD_API_KEY", "").strip()
MIN_PRICE = float(os.getenv("MIN_PRICE", 0.20))
W_MIN_VOL = float(os.getenv("W_MIN_VOL", 10000))
CACHE_DIR = "cache_eodhd"
OUT_DIR = "out"

# -------------------- Utilities --------------------
def daterange_start(years:int) -> str:
    d = dt.date.today() - dt.timedelta(days=365*years + 7)
    return d.isoformat()

def ensure_dirs():
    os.makedirs(CACHE_DIR, exist_ok=True)
    os.makedirs(OUT_DIR, exist_ok=True)

def to_float(x):
    try:
        return float(x)
    except Exception:
        return np.nan

# -------------------- Data: EODHD only --------------------
def eodhd_fetch_daily_ax(ticker_ax:str, years:int=3) -> pd.DataFrame:
    """
    Fetch daily OHLCV from EODHD for an ASX ticker like 'BHP.AX'.
    Uses /eod/{SYMBOL}.AU . Returns DataFrame columns: date, open, high, low, close, volume.
    """
    if not EODHD_API_KEY:
        raise RuntimeError("EODHD_API_KEY not set")
    symbol = ticker_ax.replace(".AX", "")
    url = f"https://eodhd.com/api/eod/{symbol}.AU"
    params = {
        "api_token": EODHD_API_KEY,
        "fmt": "json",
        "period": "d",
        "from": daterange_start(years),
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    rows = r.json()
    if not isinstance(rows, list) or not rows:
        return pd.DataFrame()
    df = pd.DataFrame(rows)
    # Expected columns: date, open, high, low, close, volume
    keep = ["date","open","high","low","close","volume"]
    for k in keep:
        if k not in df.columns:
            df[k] = np.nan
    df = df[keep].copy()
    df["date"] = pd.to_datetime(df["date"]).dt.date
    for c in ["open","high","low","close","volume"]:
        df[c] = df[c].apply(to_float)
    df["ticker"] = ticker_ax
    df.dropna(subset=["close","volume"], inplace=True)
    df.sort_values("date", inplace=True)
    df.drop_duplicates(subset=["ticker","date"], keep="last", inplace=True)
    return df.reset_index(drop=True)

def fetch_prices_for_ticker(ticker_ax:str, years:int=3, refresh:bool=False) -> pd.DataFrame:
    """
    Cached EODHD fetch. If cache exists and not refresh, return it.
    If refresh, try fetch and merge (append new rows).
    """
    ensure_dirs()
    cache_file = os.path.join(CACHE_DIR, f"{ticker_ax}_ohlc.csv")
    cached = None
    if os.path.exists(cache_file):
        try:
            cached = pd.read_csv(cache_file, parse_dates=["date"])
            cached["date"] = cached["date"].dt.date
        except Exception:
            cached = None

    if cached is not None and not refresh:
        return cached

    try:
        fresh = eodhd_fetch_daily_ax(ticker_ax, years=years)
    except Exception as e:
        # fall back to cache if available
        if cached is not None:
            return cached
        return pd.DataFrame()

    if fresh.empty and cached is not None:
        return cached

    if cached is None or cached.empty:
        out = fresh
    else:
        out = pd.concat([cached, fresh], ignore_index=True)
        out.drop_duplicates(subset=["ticker","date"], keep="last", inplace=True)
        out.sort_values("date", inplace=True)
    out.to_csv(cache_file, index=False)
    return out

# -------------------- Features & Scoring --------------------
def add_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute simple breakout features and composite Score.
    Required inputs: columns date, close, volume.
    Returns original +:
      vma20, v_ratio, ret5 (%), hh250, dist_to_hh (%), ma20, ma50, trend_up (0/1), score
    """
    p = df.copy()
    p = p.sort_values("date")
    # sanity
    for c in ["close","volume"]:
        p[c] = pd.to_numeric(p[c], errors="coerce")
    p.dropna(subset=["close","volume"], inplace=True)

    if len(p) < 60:
        return pd.DataFrame()  # not enough history

    p["vma20"] = p["volume"].rolling(20).mean()
    p["v_ratio"] = p["volume"] / (p["vma20"] + 1e-9)
    p["ret5"] = 100.0 * (p["close"] / p["close"].shift(5) - 1.0)
    p["hh250"] = p["close"].rolling(250, min_periods=50).max()
    p["dist_to_hh"] = 100.0 * (p["close"] / p["hh250"] - 1.0)
    p["ma20"] = p["close"].rolling(20).mean()
    p["ma50"] = p["close"].rolling(50).mean()
    p["trend_up"] = ((p["close"] > p["ma20"]) & (p["ma20"] > p["ma50"])).astype(int)

    # Winsorize to make Score stable
    def winsor(s, lo=0.01, hi=0.99):
        a, b = s.quantile(lo), s.quantile(hi)
        return s.clip(a, b)

    v = winsor(p["v_ratio"].replace([np.inf, -np.inf], np.nan).fillna(0), 0.01, 0.99)
    r = winsor(p["ret5"].replace([np.inf, -np.inf], np.nan).fillna(0), 0.01, 0.99)
    d = winsor((-p["dist_to_hh"]).replace([np.inf, -np.inf], np.nan).fillna(0), 0.01, 0.99)  # closer to HH better

    # Normalize to z-scores
    def zscore(s):
        m, sd = s.mean(), s.std(ddof=0)
        if sd == 0 or np.isnan(sd):
            return s*0
        return (s - m) / sd

    z_v = zscore(v)
    z_r = zscore(r)
    z_d = zscore(d)

    p["score"] = 0.45*z_v + 0.35*z_r + 0.20*z_d + 0.20*p["trend_up"]
    return p.dropna().reset_index(drop=True)

# -------------------- Universe (ASX tiers) --------------------
ASX100_FALLBACK = [
    # A broad static list (trim/extend as you like)
    "BHP.AX","CBA.AX","CSL.AX","NAB.AX","WBC.AX","ANZ.AX","WES.AX","WOW.AX","TLS.AX","MQG.AX",
    "RIO.AX","FMG.AX","GMG.AX","WDS.AX","ALL.AX","BXB.AX","QBE.AX","TCL.AX","ORI.AX","APA.AX",
    "COL.AX","ALD.AX","S32.AX","IGO.AX","REA.AX","CAR.AX","RMD.AX","STO.AX","NXT.AX","EVN.AX",
    "JHX.AX","CPU.AX","SUN.AX","MIN.AX","ARB.AX","A2M.AX","ILU.AX","ASX.AX","SEK.AX","TPG.AX",
    "DMP.AX","TWE.AX","SHL.AX","QAN.AX","AMP.AX","ORI.AX","AZJ.AX","IFL.AX","IEL.AX","PLS.AX"
]
MID_FALLBACK = [
    "ABB.AX","ABC.AX","ALQ.AX","AMC.AX","APA.AX","APX.AX","ARB.AX","ARU.AX","ASX.AX","AZJ.AX",
    "BAP.AX","BGL.AX","BKL.AX","BOQ.AX","BRG.AX","BWP.AX","CAR.AX","CDA.AX","CGC.AX","CHC.AX",
    "CIM.AX","CMW.AX","COH.AX","CPU.AX","CQR.AX","CSL.AX","CTD.AX","CWN.AX","DOW.AX","EHL.AX"
]
MICRO_FALLBACK = [
    "ART.AX","AIM.AX","BET.AX","CCX.AX","CTT.AX","DSK.AX","ATP.AX","AHX.AX","AD8.AX","AVZ.AX",
    "PNV.AX","WBT.AX","NAN.AX","PSQ.AX","ALA.AX","SXY.AX","RZI.AX","OCL.AX","WAF.AX","DDR.AX"
]

def eodhd_constituents(index_code:str) -> list:
    if not EODHD_API_KEY:
        return []
    try:
        url = f"https://eodhd.com/api/index-constituents/{index_code}"
        r = requests.get(url, params={"api_token": EODHD_API_KEY, "fmt": "json"}, timeout=20)
        r.raise_for_status()
        rows = r.json()
        out = []
        for row in rows:
            code = (row.get("Code") or row.get("code") or "").strip()
            exch = (row.get("Exchange") or row.get("exchange") or "").strip()
            if not code:
                continue
            if not code.endswith(".AX"):
                if exch in ("XASX","ASX","AU"):
                    code = code + ".AX"
            out.append(code)
        return [t for t in out if t.endswith(".AX")]
    except Exception:
        return []

def build_tiered_universe(uni_cap:int=0) -> dict:
    """
    Try ASX200 via EODHD (XJO.AU). If available, large=head(100), mid=next(100).
    Micro = curated fallback.
    If not, fall back to static lists. Apply cap if uni_cap > 0.
    """
    large = eodhd_constituents("XJO.AU")
    if len(large) >= 120:
        asx200 = large[:200]
        large = asx200[:100]
        mid   = asx200[100:200]
    else:
        # fallback
        large = list(dict.fromkeys(ASX100_FALLBACK))
        mid   = [t for t in MID_FALLBACK if t not in large]

    micro = [t for t in MICRO_FALLBACK if t not in large and t not in mid]

    if uni_cap and uni_cap > 0:
        large = large[:uni_cap]
        mid   = mid[:uni_cap]
        micro = micro[:uni_cap]

    if not large: large = ASX100_FALLBACK[: max(1, uni_cap or 10)]
    if not mid:   mid   = MID_FALLBACK[:   max(1, uni_cap or 10)]
    if not micro: micro = MICRO_FALLBACK[: max(1, uni_cap or 10)]

    return {"large": large, "mid": mid, "micro": micro}

# -------------------- Scanning --------------------
def scan_tier(tier_name:str, tickers:list, years:int, topN:int, refresh:bool=False) -> pd.DataFrame:
    rows = []
    for t in tqdm(tickers, desc=f"{tier_name.capitalize()} scan", unit="stk"):
        df = fetch_prices_for_ticker(t, years=years, refresh=refresh)
        if df is None or df.empty:
            continue
        # Liquidity filters on last row
        last = df.iloc[-1]
        if to_float(last["close"]) < MIN_PRICE:
            continue
        v20 = df["volume"].rolling(20).mean().iloc[-1]
        if np.isnan(v20) or v20 < W_MIN_VOL:
            continue

        f = add_features(df)
        if f is None or f.empty:
            continue
        lf = f.iloc[-1]
        rows.append({
            "Tier": tier_name,
            "Ticker": t,
            "Date": lf["date"],
            "Close": to_float(lf["close"]),
            "Score": to_float(lf["score"]),
            "VRatio": to_float(lf.get("v_ratio", np.nan)),
            "Ret5": to_float(lf.get("ret5", np.nan)),
            "DistToHH": to_float(lf.get("dist_to_hh", np.nan)),
            "TrendUp": int(lf.get("trend_up", 0)),
            "AvgVol20": to_float(lf.get("vma20", np.nan)),
        })
    df_all = pd.DataFrame(rows)
    if df_all.empty:
        print(f"Warning: no candidates in tier '{tier_name}'")
        return df_all
    top = (df_all.sort_values(["Score","VRatio","Ret5"], ascending=[False,False,False])
                 .head(topN)
                 .reset_index(drop=True))
    return top

def pretty(df: pd.DataFrame, title: str):
    if df is None or df.empty:
        print(f"\n=== {title} (no rows) ===")
        return
    print(f"\n=== {title} (Top {len(df)}) ===")
    fmts = {
        "Close": lambda x: f"{x:.2f}",
        "Score": lambda x: f"{x:.3f}",
        "VRatio":lambda x: f"{x:.2f}",
        "Ret5":  lambda x: f"{x:.2f}%",
        "DistToHH": lambda x: f"{x:.2f}%"
    }
    print(df.to_string(index=False, formatters=fmts))

# -------------------- Main --------------------
def main():
    ap = ArgumentParser()
    ap.add_argument("--years", type=int, default=int(os.getenv("YEARS", 3)))
    ap.add_argument("--topN", type=int, default=int(os.getenv("TOPN", 10)))
    ap.add_argument("--universe", type=int, default=int(os.getenv("UNIVERSE_CAP", 0)))
    ap.add_argument("--refresh", action="store_true", help="force pulling from API and merge into cache")
    args = ap.parse_args()

    ensure_dirs()
    print(f"Config → years={args.years} | topN={args.topN} | uni_cap={args.universe}")

    uni = build_tiered_universe(uni_cap=args.universe)
    print(f"Universe sizes → large={len(uni['large'])} | mid={len(uni['mid'])} | micro={len(uni['micro'])}")

    large = scan_tier("large", uni["large"], args.years, args.topN, refresh=args.refresh)
    mid   = scan_tier("mid",   uni["mid"],   args.years, args.topN, refresh=args.refresh)
    micro = scan_tier("micro", uni["micro"], args.years, args.topN, refresh=args.refresh)

    pretty(large, "Large Caps")
    pretty(mid,   "Mid Caps")
    pretty(micro, "Micro Caps")

    # Save
    if not large.empty: large.to_csv(os.path.join(OUT_DIR,"tier_large.csv"), index=False)
    if not mid.empty:   mid.to_csv(os.path.join(OUT_DIR,"tier_mid.csv"), index=False)
    if not micro.empty: micro.to_csv(os.path.join(OUT_DIR,"tier_micro.csv"), index=False)

    combined_parts = [x for x in [large, mid, micro] if x is not None and not x.empty]
    if not combined_parts:
        print("\nNo combined results.")
        return
    combined = (pd.concat(combined_parts, ignore_index=True)
                  .sort_values(["Score","VRatio","Ret5"], ascending=[False,False,False])
                  .head(args.topN)
                  .reset_index(drop=True))
    print("\n=== Top Combined (by Score/VRatio/Ret5) ===")
    print(combined.to_string(index=False, formatters={
        "Close":lambda x:f"{x:.2f}",
        "Score":lambda x:f"{x:.3f}",
        "VRatio":lambda x:f"{x:.2f}",
        "Ret5":lambda x:f"{x:.2f}%",
        "DistToHH":lambda x:f"{x:.2f}%",
    }))
    combined.to_csv(os.path.join(OUT_DIR,"tier_combined.csv"), index=False)

if __name__ == "__main__":
    main()
